{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# License: BSD\n",
    "# Author: Ghassen Hamrouni\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root='.', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])), batch_size=64, shuffle=True, num_workers=4)\n",
    "# Test dataset\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root='.', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])), batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 3 * 3, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 3 * 3)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transform the input\n",
    "        x = self.stn(x)\n",
    "\n",
    "        # Perform the usual forward pass\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.300849\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.030376\n",
      "\n",
      "Test set: Average loss: 0.2591, Accuracy: 9227/10000 (92%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.477124\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.264399\n",
      "\n",
      "Test set: Average loss: 0.3328, Accuracy: 8893/10000 (89%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.715932\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.428384\n",
      "\n",
      "Test set: Average loss: 0.1011, Accuracy: 9714/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.197246\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.225891\n",
      "\n",
      "Test set: Average loss: 0.1043, Accuracy: 9681/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.338069\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.101722\n",
      "\n",
      "Test set: Average loss: 0.0797, Accuracy: 9759/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.300147\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.367683\n",
      "\n",
      "Test set: Average loss: 0.0869, Accuracy: 9740/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.084209\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.216674\n",
      "\n",
      "Test set: Average loss: 0.0876, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.062870\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.240506\n",
      "\n",
      "Test set: Average loss: 0.1045, Accuracy: 9673/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.194266\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.071273\n",
      "\n",
      "Test set: Average loss: 2.2022, Accuracy: 5954/10000 (60%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 2.456358\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.215875\n",
      "\n",
      "Test set: Average loss: 0.0503, Accuracy: 9840/10000 (98%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.102080\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.160600\n",
      "\n",
      "Test set: Average loss: 0.0461, Accuracy: 9854/10000 (99%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.063451\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.148191\n",
      "\n",
      "Test set: Average loss: 0.0702, Accuracy: 9787/10000 (98%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.066380\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.026276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-100:\n",
      "Process Process-99:\n",
      "Process Process-97:\n",
      "Process Process-98:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in default_collate\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 113, in default_collate\n",
      "    storage = batch[0].storage()._new_shared(numel)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torch/storage.py\", line 116, in _new_shared\n",
      "    return cls._new_using_fd(size)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 77, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 76, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 82, in to_tensor\n",
      "    if isinstance(img, torch.ByteTensor):\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "KeyboardInterrupt\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 77, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 77, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 76, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 70, in to_tensor\n",
      "    img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 76, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 83, in to_tensor\n",
      "    return img.float().div(255)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-e8d018404dec>\", line 102, in <module>\n",
      "    train(epoch)\n",
      "  File \"<ipython-input-5-e8d018404dec>\", line 6, in train\n",
      "    for batch_idx, (data, target) in enumerate(train_loader):\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 272, in __next__\n",
      "    return self._process_next_batch(batch)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 305, in _process_next_batch\n",
      "    self._put_indices()\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 298, in _put_indices\n",
      "    self.index_queues[self.worker_queue_idx].put((self.send_idx, indices))\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 347, in put\n",
      "    self._writer.send_bytes(obj)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/inspect.py\", line 1445, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 177, in findsource\n",
      "    lines = linecache.getlines(file, globals_dict)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/linecache.py\", line 137, in updatecache\n",
      "    lines = fp.readlines()\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/codecs.py\", line 318, in decode\n",
      "    def decode(self, input, final=False):\n",
      "  File \"/home/imatge_upc_ai/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 178, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 19630) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 500 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "                total_loss.append(loss.item)\n",
    "            \n",
    "#\n",
    "# A simple test procedure to measure STN the performances on MNIST.\n",
    "#\n",
    "\n",
    "\n",
    "def test():\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            # sum up batch loss\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "              .format(test_loss, correct, len(test_loader.dataset),\n",
    "                      100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "######################################################################\n",
    "# Visualizing the STN results\n",
    "# ---------------------------\n",
    "#\n",
    "# Now, we will inspect the results of our learned visual attention\n",
    "# mechanism.\n",
    "#\n",
    "# We define a small helper function in order to visualize the\n",
    "# transformations while training.\n",
    "\n",
    "\n",
    "def convert_image_np(inp):\n",
    "    \"\"\"Convert a Tensor to numpy image.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    return inp\n",
    "\n",
    "# We want to visualize the output of the spatial transformers layer\n",
    "# after the training, we visualize a batch of input images and\n",
    "# the corresponding transformed batch using STN.\n",
    "\n",
    "\n",
    "def visualize_stn():\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of training data\n",
    "        data = next(iter(test_loader))[0].to(device)\n",
    "\n",
    "        input_tensor = data.cpu()\n",
    "        transformed_input_tensor = model.stn(data).cpu()\n",
    "\n",
    "        in_grid = convert_image_np(\n",
    "            torchvision.utils.make_grid(input_tensor))\n",
    "\n",
    "        out_grid = convert_image_np(\n",
    "            torchvision.utils.make_grid(transformed_input_tensor))\n",
    "\n",
    "        scipy.misc.imsave('in_grid.png', in_grid)\n",
    "        scipy.misc.imsave('out_grid.png', out_grid)\n",
    "        torchvision.utils.save_image(input_tensor, \"in_grid_img.png\", nrow=8, padding=2, normalize=False, range=None, scale_each=False,\n",
    "                                     pad_value=0)\n",
    "\n",
    "        torchvision.utils.save_image(transformed_input_tensor, \"out_grid_img.png\", nrow=8, padding=2, normalize=False, range=None,\n",
    "                                     scale_each=False,\n",
    "                                     pad_value=0)\n",
    "\n",
    "\n",
    "        # # Plot the results side-by-side\n",
    "        # f, axarr = plt.subplots(1, 2)\n",
    "        # axarr[0].imshow(in_grid)\n",
    "        # axarr[0].set_title('Dataset Images')\n",
    "        #\n",
    "        # axarr[1].imshow(out_grid)\n",
    "        # axarr[1].set_title('Transformed Images')\n",
    "\n",
    "\n",
    "for epoch in range(1, 20 + 1):\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "\n",
    "# Visualize the STN transformation on some input batch\n",
    "visualize_stn()\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
